{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42ae8255-7c90-48b3-b742-b718a1d7d77b",
   "metadata": {},
   "source": [
    "## Stability measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed0d403a-4ff8-438a-94da-ddcb3aa40d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    "    \"etbert\": \"../data/etbert/synth_emb.pkl\",\n",
    "    \"yatc\": \"../data/yatc/synth_emb.pkl\",\n",
    "    \"netmamba\": \"../data/netmamba/netmamba_synth_emb.pkl\",\n",
    "    \"netfound\": \"../data/netfound/synth_emb.pkl\",\n",
    "}\n",
    "\n",
    "mapping = lambda x: x.split(\"/\")[-1].split(\"exp\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7b5c1d4-e98e-4e23-9105-445086b02b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pickle\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def measure_spread(tensor1, tensor2):\n",
    "    similarity_matrix = F.cosine_similarity(tensor1.unsqueeze(1), tensor2.unsqueeze(0), dim=2)\n",
    "    return similarity_matrix.mean()\n",
    "\n",
    "def measure_dist(tensor1: torch.Tensor, tensor2: torch.Tensor) -> float:\n",
    "    diff = torch.abs(tensor1.unsqueeze(1) - tensor2.unsqueeze(0))\n",
    "    distances = diff.sum(dim=2)\n",
    "    return distances.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f54d13-b195-4ba8-b307-b020ed414dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_probing(embeddings, labels, test_size=0.2, random_state=42):\n",
    "    import torch\n",
    "    import numpy as np\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "    # Convert embeddings tensor to numpy array\n",
    "    X = embeddings.cpu().numpy()\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    # Ensure exactly two classes exist\n",
    "    unique_classes = np.unique(y)\n",
    "    if len(unique_classes) != 2:\n",
    "        raise ValueError(\"There must be exactly two classes in the labels.\")\n",
    "    \n",
    "    # Map string labels to binary integers\n",
    "    class_to_int = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "    y_int = np.array([class_to_int[label] for label in y])\n",
    "    \n",
    "    # Shuffle and split dataset into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y_int, test_size=test_size, random_state=random_state, shuffle=True\n",
    "    )\n",
    "    \n",
    "    # Train logistic regression classifier\n",
    "    clf = LogisticRegression(random_state=random_state, max_iter=1000)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate classifier performance\n",
    "    y_train_pred = clf.predict(X_train)\n",
    "    y_test_pred = clf.predict(X_test)\n",
    "    \n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred)\n",
    "    test_f1 = f1_score(y_test, y_test_pred)\n",
    "    \n",
    "    return train_f1, test_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b5f72fc-62ca-4871-9e05-01d96a7051aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model etbert, baseline stability: avg_cosine_similarity = 0.9698, avg_L1_dist = 155\n",
      "Model etbert, class fifo_6m_cubic_prof50_36_: avg_cosine_similarity = 0.9703, avg_L1_dist = 154, Train F1 Score: 1.0000, Test F1 Score: 0.4286\n",
      "Model etbert, class codel_6m_bbr_prof50_36_: avg_cosine_similarity = 0.9673, avg_L1_dist = 161, Train F1 Score: 0.9756, Test F1 Score: 0.6829\n",
      "Model etbert, class fifo_6m_bbr_prof72_29_: avg_cosine_similarity = 0.9708, avg_L1_dist = 153, Train F1 Score: 0.9929, Test F1 Score: 0.4286\n",
      "Model etbert, class codel_6m_cubic_prof72_29_: avg_cosine_similarity = 0.9679, avg_L1_dist = 160, Train F1 Score: 0.9818, Test F1 Score: 0.4615\n",
      "Model yatc, baseline stability: avg_cosine_similarity = 0.8939, avg_L1_dist = 28\n",
      "Model yatc, class fifo_6m_cubic_prof50_36_: avg_cosine_similarity = 0.8899, avg_L1_dist = 28, Train F1 Score: 0.6885, Test F1 Score: 0.4828\n",
      "Model yatc, class codel_6m_bbr_prof50_36_: avg_cosine_similarity = 0.8942, avg_L1_dist = 28, Train F1 Score: 0.7719, Test F1 Score: 0.5128\n",
      "Model yatc, class fifo_6m_bbr_prof72_29_: avg_cosine_similarity = 0.8922, avg_L1_dist = 28, Train F1 Score: 0.6977, Test F1 Score: 0.2424\n",
      "Model yatc, class codel_6m_cubic_prof72_29_: avg_cosine_similarity = 0.8960, avg_L1_dist = 28, Train F1 Score: 0.7515, Test F1 Score: 0.4737\n",
      "Model netmamba, baseline stability: avg_cosine_similarity = 0.9940, avg_L1_dist = 21\n",
      "Model netmamba, class fifo_6m_cubic_prof50_36_: avg_cosine_similarity = 0.9942, avg_L1_dist = 21, Train F1 Score: 0.4554, Test F1 Score: 0.2857\n",
      "Model netmamba, class codel_6m_bbr_prof50_36_: avg_cosine_similarity = 0.9097, avg_L1_dist = 41, Train F1 Score: 0.7010, Test F1 Score: 0.7037\n",
      "Model netmamba, class fifo_6m_bbr_prof72_29_: avg_cosine_similarity = 0.9942, avg_L1_dist = 21, Train F1 Score: 0.5606, Test F1 Score: 0.3333\n",
      "Model netmamba, class codel_6m_cubic_prof72_29_: avg_cosine_similarity = 0.9813, avg_L1_dist = 24, Train F1 Score: 0.6243, Test F1 Score: 0.6190\n",
      "Model netfound, baseline stability: avg_cosine_similarity = 0.9700, avg_L1_dist = 121\n",
      "Model netfound, class fifo_6m_cubic_prof50_36_: avg_cosine_similarity = 0.9669, avg_L1_dist = 127, Train F1 Score: 0.9242, Test F1 Score: 0.6000\n",
      "Model netfound, class codel_6m_bbr_prof50_36_: avg_cosine_similarity = 0.9515, avg_L1_dist = 149, Train F1 Score: 0.9750, Test F1 Score: 0.9302\n",
      "Model netfound, class fifo_6m_bbr_prof72_29_: avg_cosine_similarity = 0.9591, avg_L1_dist = 137, Train F1 Score: 0.9231, Test F1 Score: 0.7778\n",
      "Model netfound, class codel_6m_cubic_prof72_29_: avg_cosine_similarity = 0.9173, avg_L1_dist = 195, Train F1 Score: 0.9940, Test F1 Score: 0.9744\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "experiments:\n",
    "'fifo_6m_bbr_prof50_36_' - baseline\n",
    "'fifo_6m_cubic_prof50_36_' - different cc algorithm\n",
    "'codel_6m_bbr_prof50_36_' - different AQM\n",
    "'fifo_6m_bbr_prof72_29_' - different cross traffic\n",
    "'codel_6m_cubic_prof72_29_' - different cc, aqm, and cross traffic\n",
    "\"\"\"\n",
    "\n",
    "baseline = 'fifo_6m_bbr_prof50_36_'\n",
    "others = {\n",
    "    'fifo_6m_cubic_prof50_36_': \"Congestion Control\",\n",
    "    'codel_6m_bbr_prof50_36_': \"AQM\",\n",
    "    'fifo_6m_bbr_prof72_29_': \"Crosstraffic\",\n",
    "    'codel_6m_cubic_prof72_29_': \"All\",\n",
    "}\n",
    "\n",
    "results = {\n",
    "    'yatc': {\n",
    "        'Average': 0.8633,\n",
    "    },\n",
    "    'etbert': {\n",
    "        'Average': 0.7977,\n",
    "    },\n",
    "    'netmamba': {\n",
    "        'Average': 0.9639,\n",
    "    },\n",
    "    'netfound': {\n",
    "        'Average': 0.8017,\n",
    "    },\n",
    "}\n",
    "\n",
    "for model, path in data.items():\n",
    "    with open(path, \"rb\") as f:\n",
    "        embeddings, labels = pickle.load(f)\n",
    "    labels = [mapping(x) for x in labels]\n",
    "    assert isinstance(embeddings, torch.Tensor)\n",
    "    assert embeddings.size(0) == len(labels)\n",
    "    classes = {\n",
    "        x: embeddings[torch.tensor([label == x for label in labels], dtype=torch.bool)]\n",
    "        for x in set(labels)\n",
    "    }\n",
    "\n",
    "    # self-similarity for baseline - stability test\n",
    "    results[model]['Stability baseline'] = measure_spread(classes[baseline], classes[baseline]).item()\n",
    "    print(f\"Model {model}, baseline stability: avg_cosine_similarity = {results[model]['Stability baseline']:.4f}, avg_L1_dist = {measure_dist(classes[baseline], classes[baseline]):.0f}\")\n",
    "\n",
    "    # similarity of others\n",
    "    for x in others:\n",
    "        avg_cos_sim = measure_spread(classes[baseline], classes[x])\n",
    "        results[model][others[x]] = avg_cos_sim.item()\n",
    "        avg_dist = measure_dist(classes[baseline], classes[x])\n",
    "        f1_train, f1_test = linear_probing(\n",
    "            embeddings=torch.cat([classes[baseline], classes[x]]),\n",
    "            labels=[baseline] * classes[baseline].size(0) + [x] * classes[x].size(0),\n",
    "        )\n",
    "        results[model][others[x] + \"_f1\"] = f1_test\n",
    "        print(f\"Model {model}, class {x}: avg_cosine_similarity = {avg_cos_sim:.4f}, avg_L1_dist = {avg_dist:.0f}, Train F1 Score: {f1_train:.4f}, Test F1 Score: {f1_test:.4f}\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8388aef-27b1-467b-a25d-c5a12ce39d70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yatc': {'Average': 0.8633,\n",
       "  'Stability baseline': 0.8939048051834106,\n",
       "  'Congestion Control': 0.8899036049842834,\n",
       "  'Congestion Control_f1': 0.4827586206896552,\n",
       "  'AQM': 0.8942065238952637,\n",
       "  'AQM_f1': 0.5128205128205128,\n",
       "  'Crosstraffic': 0.8921573162078857,\n",
       "  'Crosstraffic_f1': 0.24242424242424243,\n",
       "  'All': 0.895961344242096,\n",
       "  'All_f1': 0.47368421052631576},\n",
       " 'etbert': {'Average': 0.7977,\n",
       "  'Stability baseline': 0.9697624444961548,\n",
       "  'Congestion Control': 0.9703209400177002,\n",
       "  'Congestion Control_f1': 0.42857142857142855,\n",
       "  'AQM': 0.9672881364822388,\n",
       "  'AQM_f1': 0.6829268292682927,\n",
       "  'Crosstraffic': 0.9707930684089661,\n",
       "  'Crosstraffic_f1': 0.42857142857142855,\n",
       "  'All': 0.967918336391449,\n",
       "  'All_f1': 0.46153846153846156},\n",
       " 'netmamba': {'Average': 0.9639,\n",
       "  'Stability baseline': 0.9940047264099121,\n",
       "  'Congestion Control': 0.9941560626029968,\n",
       "  'Congestion Control_f1': 0.2857142857142857,\n",
       "  'AQM': 0.9097089171409607,\n",
       "  'AQM_f1': 0.7037037037037037,\n",
       "  'Crosstraffic': 0.9941582679748535,\n",
       "  'Crosstraffic_f1': 0.3333333333333333,\n",
       "  'All': 0.9812585711479187,\n",
       "  'All_f1': 0.6190476190476191},\n",
       " 'netfound': {'Average': 0.8017,\n",
       "  'Stability baseline': 0.9699736833572388,\n",
       "  'Congestion Control': 0.9668754935264587,\n",
       "  'Congestion Control_f1': 0.6,\n",
       "  'AQM': 0.9515206217765808,\n",
       "  'AQM_f1': 0.9302325581395349,\n",
       "  'Crosstraffic': 0.9591284394264221,\n",
       "  'Crosstraffic_f1': 0.7777777777777778,\n",
       "  'All': 0.9172675609588623,\n",
       "  'All_f1': 0.9743589743589743}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b0230e9-67c7-4a0a-b238-7ec8e98a22a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LaTeX rows for DataFrame 1 (Delta from Stability):\n",
      "%\t-0.0040 & 0.4828 & 0.0006 & 0.4286 & -0.0031 & 0.6000 & 0.0002 & 0.2857 \\\\\n",
      "%\t0.0003 & 0.5128 & -0.0025 & 0.6829 & -0.0185 & 0.9302 & -0.0843 & 0.7037 \\\\\n",
      "%\t-0.0017 & 0.2424 & 0.0010 & 0.4286 & -0.0108 & 0.7778 & 0.0002 & 0.3333 \\\\\n",
      "%\t0.0021 & 0.4737 & -0.0018 & 0.4615 & -0.0527 & 0.9744 & -0.0127 & 0.6190 \\\\\n",
      "\n",
      "LaTeX rows for DataFrame 2 (Normalized Delta in %):\n",
      "%\t-13.07\\% & 0.48 & 0.32\\% & 0.43 & -1.84\\% & 0.60 & 0.50\\% & 0.29 \\\\\n",
      "%\t0.99\\% & 0.51 & -1.44\\% & 0.68 & -10.97\\% & 0.93 & -280.01\\% & 0.70 \\\\\n",
      "%\t-5.71\\% & 0.24 & 0.60\\% & 0.43 & -6.45\\% & 0.78 & 0.51\\% & 0.33 \\\\\n",
      "%\t6.72\\% & 0.47 & -1.07\\% & 0.46 & -31.32\\% & 0.97 & -42.34\\% & 0.62 \\\\\n"
     ]
    }
   ],
   "source": [
    "model_order = ['yatc', 'etbert', 'netfound', 'netmamba']\n",
    "row_order = ['Congestion Control', 'AQM', 'Crosstraffic', 'All']\n",
    "cos_results = {x: {\n",
    "    y: results[x][y] for y in row_order\n",
    "} for x in model_order}\n",
    "f1_results = {x + \"_f1\": {\n",
    "    y: results[x].get(y + \"_f1\", '-') for y in row_order\n",
    "} for x in model_order}\n",
    "df_last3_copy = pd.DataFrame(cos_results | f1_results)[['yatc', 'yatc_f1', 'etbert', 'etbert_f1', 'netfound', 'netfound_f1', 'netmamba', 'netmamba_f1']]\n",
    "df_last3_copy\n",
    "\n",
    "average = {x: results[x]['Average'] for x in model_order}\n",
    "stability = {x: results[x]['Stability baseline'] for x in model_order}\n",
    "\n",
    "# --- DataFrame 1: Delta from Stability (sign reversed: value - stability) ---\n",
    "# Copy all columns from the original last three rows, update only odd columns.\n",
    "df_delta_all = df_last3_copy.copy()\n",
    "for col in model_order:\n",
    "    df_delta_all[col] = df_last3_copy[col] - stability[col]\n",
    "\n",
    "# --- DataFrame 2: Normalized Delta in Percentage (sign reversed) ---\n",
    "# Again, copy all columns, and update only the odd ones.\n",
    "df_norm_all = df_last3_copy.copy()\n",
    "for col in model_order:\n",
    "    # The denominator is (stability - average)\n",
    "    range_val = stability[col] - average[col]\n",
    "    df_norm_all[col] = ((df_last3_copy[col] - stability[col]) / range_val) * 100\n",
    "\n",
    "# --- Helper function to convert a DataFrame to LaTeX table rows ---\n",
    "def df_to_latex_rows(df, percent_cols=None, float_format=\"{:.4f}\"):\n",
    "    \"\"\"\n",
    "    Converts each row of the DataFrame into a LaTeX table row string.\n",
    "    :param df: DataFrame whose rows will be converted.\n",
    "    :param percent_cols: List of columns that should be formatted as percentages.\n",
    "    :param float_format: Format string for numeric values.\n",
    "    \"\"\"\n",
    "    if percent_cols is None:\n",
    "        percent_cols = []\n",
    "    latex_lines = []\n",
    "    for i, row in df.iterrows():\n",
    "        row_values = []\n",
    "        for col in df.columns:\n",
    "            if col == 'Metric':\n",
    "                row_values.append(str(row[col]))\n",
    "            else:\n",
    "                if col in percent_cols:\n",
    "                    try:\n",
    "                        # Format as percentage with 2 decimal places and add a percent sign.\n",
    "                        formatted_val = f\"{row[col]:.2f}\\\\%\"\n",
    "                    except Exception as e:\n",
    "                        formatted_val = str(row[col])\n",
    "                else:\n",
    "                    try:\n",
    "                        formatted_val = float_format.format(row[col])\n",
    "                    except Exception as e:\n",
    "                        formatted_val = str(row[col])\n",
    "                row_values.append(formatted_val)\n",
    "        # Build the LaTeX row with a '%' at the beginning (like your input)\n",
    "        latex_line = \"%\\t\" + \" & \".join(row_values) + \" \\\\\\\\\"\n",
    "        latex_lines.append(latex_line)\n",
    "    return \"\\n\".join(latex_lines)\n",
    "\n",
    "# --- Generate LaTeX rows ---\n",
    "# For DataFrame 1, odd columns are standard numeric values.\n",
    "latex_df_delta = df_to_latex_rows(df_delta_all, percent_cols=[], float_format=\"{:.4f}\")\n",
    "\n",
    "# For DataFrame 2, format odd columns as percentages.\n",
    "latex_df_norm = df_to_latex_rows(df_norm_all, percent_cols=model_order, float_format=\"{:.2f}\")\n",
    "\n",
    "# --- Print the LaTeX rows ---\n",
    "print(\"LaTeX rows for DataFrame 1 (Delta from Stability):\")\n",
    "print(latex_df_delta)\n",
    "print(\"\\nLaTeX rows for DataFrame 2 (Normalized Delta in %):\")\n",
    "print(latex_df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "022dd11f-10ae-4632-a5c9-269ebfada92e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yatc': 0.8633, 'etbert': 0.7977, 'netfound': 0.8017, 'netmamba': 0.9639}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e36638c6-d884-4e54-bc49-4a99e2b1e812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'yatc': 0.8939048051834106,\n",
       " 'etbert': 0.9697624444961548,\n",
       " 'netfound': 0.9699736833572388,\n",
       " 'netmamba': 0.9940047264099121}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
