{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45d772e0-d6e6-4280-8e69-709fe40e2312",
   "metadata": {},
   "source": [
    "# YaTC feature perturbation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430f4c25-24b9-48cd-8966-941ac500c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7168e4be-c1f1-4432-998b-252ad3b3ef94",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# define CKA\n",
    "\n",
    "def gram_linear(x):\n",
    "    \"\"\"Compute Gram (kernel) matrix for a linear kernel.\n",
    "\n",
    "    Args:\n",
    "        x: A num_examples x num_features matrix of features.\n",
    "\n",
    "    Returns:\n",
    "        A num_examples x num_examples Gram matrix of examples.\n",
    "    \"\"\"\n",
    "    return x.dot(x.T)\n",
    "\n",
    "\n",
    "def gram_rbf(x, threshold=1.0):\n",
    "    \"\"\"Compute Gram (kernel) matrix for an RBF kernel.\n",
    "\n",
    "    Args:\n",
    "        x: A num_examples x num_features matrix of features.\n",
    "        threshold: Fraction of median Euclidean distance to use as RBF kernel\n",
    "        bandwidth. (This is the heuristic we use in the paper. There are other\n",
    "        possible ways to set the bandwidth; we didn't try them.)\n",
    "\n",
    "    Returns:\n",
    "        A num_examples x num_examples Gram matrix of examples.\n",
    "    \"\"\"\n",
    "    dot_products = x.dot(x.T)\n",
    "    sq_norms = np.diag(dot_products)\n",
    "    sq_distances = -2 * dot_products + sq_norms[:, None] + sq_norms[None, :]\n",
    "    sq_median_distance = np.median(sq_distances)\n",
    "    return np.exp(-sq_distances / (2 * threshold ** 2 * sq_median_distance))\n",
    "\n",
    "\n",
    "def center_gram(gram, unbiased=False):\n",
    "    \"\"\"Center a symmetric Gram matrix.\n",
    "\n",
    "    This is equvialent to centering the (possibly infinite-dimensional) features\n",
    "    induced by the kernel before computing the Gram matrix.\n",
    "\n",
    "    Args:\n",
    "        gram: A num_examples x num_examples symmetric matrix.\n",
    "        unbiased: Whether to adjust the Gram matrix in order to compute an unbiased\n",
    "        estimate of HSIC. Note that this estimator may be negative.\n",
    "\n",
    "    Returns:\n",
    "        A symmetric matrix with centered columns and rows.\n",
    "    \"\"\"\n",
    "    if not np.allclose(gram, gram.T):\n",
    "        raise ValueError('Input must be a symmetric matrix.')\n",
    "    gram = gram.copy()\n",
    "\n",
    "    if unbiased:\n",
    "        # This formulation of the U-statistic, from Szekely, G. J., & Rizzo, M.\n",
    "        # L. (2014). Partial distance correlation with methods for dissimilarities.\n",
    "        # The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically\n",
    "        # stable than the alternative from Song et al. (2007).\n",
    "        n = gram.shape[0]\n",
    "        np.fill_diagonal(gram, 0)\n",
    "        means = np.sum(gram, 0, dtype=np.float64) / (n - 2)\n",
    "        means -= np.sum(means) / (2 * (n - 1))\n",
    "        gram -= means[:, None]\n",
    "        gram -= means[None, :]\n",
    "        np.fill_diagonal(gram, 0)\n",
    "    else:\n",
    "        means = np.mean(gram, 0, dtype=np.float64)\n",
    "        means -= np.mean(means) / 2\n",
    "        gram -= means[:, None]\n",
    "        gram -= means[None, :]\n",
    "\n",
    "    return gram\n",
    "\n",
    "\n",
    "def cka(gram_x, gram_y, debiased=False):\n",
    "    \"\"\"Compute CKA.\n",
    "\n",
    "    Args:\n",
    "        gram_x: A num_examples x num_examples Gram matrix.\n",
    "        gram_y: A num_examples x num_examples Gram matrix.\n",
    "        debiased: Use unbiased estimator of HSIC. CKA may still be biased.\n",
    "\n",
    "    Returns:\n",
    "        The value of CKA between X and Y.\n",
    "    \"\"\"\n",
    "    gram_x = center_gram(gram_x, unbiased=debiased)\n",
    "    gram_y = center_gram(gram_y, unbiased=debiased)\n",
    "\n",
    "    # Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or\n",
    "    # n*(n-3) (unbiased variant), but this cancels for CKA.\n",
    "    scaled_hsic = gram_x.ravel().dot(gram_y.ravel())\n",
    "\n",
    "    normalization_x = np.linalg.norm(gram_x)\n",
    "    normalization_y = np.linalg.norm(gram_y)\n",
    "    return scaled_hsic / (normalization_x * normalization_y)\n",
    "\n",
    "\n",
    "def _debiased_dot_product_similarity_helper(\n",
    "    xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y,\n",
    "    n):\n",
    "  \"\"\"Helper for computing debiased dot product similarity (i.e. linear HSIC).\"\"\"\n",
    "  # This formula can be derived by manipulating the unbiased estimator from\n",
    "  # Song et al. (2007).\n",
    "  return (\n",
    "      xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)\n",
    "      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))\n",
    "\n",
    "\n",
    "def feature_space_linear_cka(features_x, features_y, debiased=False):\n",
    "    \"\"\"Compute CKA with a linear kernel, in feature space.\n",
    "\n",
    "    This is typically faster than computing the Gram matrix when there are fewer\n",
    "    features than examples.\n",
    "\n",
    "    Args:\n",
    "        features_x: A num_examples x num_features matrix of features.\n",
    "        features_y: A num_examples x num_features matrix of features.\n",
    "        debiased: Use unbiased estimator of dot product similarity. CKA may still be\n",
    "        biased. Note that this estimator may be negative.\n",
    "\n",
    "    Returns:\n",
    "        The value of CKA between X and Y.\n",
    "    \"\"\"\n",
    "    features_x = features_x - np.mean(features_x, 0, keepdims=True)\n",
    "    features_y = features_y - np.mean(features_y, 0, keepdims=True)\n",
    "\n",
    "    dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2\n",
    "    normalization_x = np.linalg.norm(features_x.T.dot(features_x))\n",
    "    normalization_y = np.linalg.norm(features_y.T.dot(features_y))\n",
    "\n",
    "    if debiased:\n",
    "        n = features_x.shape[0]\n",
    "        # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.\n",
    "        sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)\n",
    "        sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)\n",
    "        squared_norm_x = np.sum(sum_squared_rows_x)\n",
    "        squared_norm_y = np.sum(sum_squared_rows_y)\n",
    "\n",
    "        dot_product_similarity = _debiased_dot_product_similarity_helper(\n",
    "            dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,\n",
    "            squared_norm_x, squared_norm_y, n)\n",
    "        normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "            normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,\n",
    "            squared_norm_x, squared_norm_x, n))\n",
    "        normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "            normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,\n",
    "            squared_norm_y, squared_norm_y, n))\n",
    "\n",
    "    return dot_product_similarity / (normalization_x * normalization_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb67d239-5dd1-4006-88d6-e3e8db03e3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 12096\n",
    "LIMIT = 300000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a966f8-411b-4b1e-86f7-bcc592df9e44",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pscratch/sd/k/kell/demystifying/YaTC/src\n"
     ]
    }
   ],
   "source": [
    "# change folder to yatc source \n",
    "\n",
    "import pickle\n",
    "from tqdm.auto import tqdm\n",
    "import copy\n",
    "import threading\n",
    "\n",
    "from finetune import build_dataset\n",
    "import torch\n",
    "import models_YaTC\n",
    "from util.pos_embed import interpolate_pos_embed\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_loader(datafolder, batch_size=1):\n",
    "    loader_yatc = lambda: True\n",
    "    loader_yatc.data_path = datafolder\n",
    "    loader_yatc = build_dataset(is_train=True, args=loader_yatc)\n",
    "    loader_yatc = torch.utils.data.DataLoader(\n",
    "            loader_yatc, sampler=torch.utils.data.SequentialSampler(loader_yatc),\n",
    "            batch_size=batch_size,\n",
    "            num_workers=4,\n",
    "            pin_memory=False,\n",
    "            drop_last=True,\n",
    "        )\n",
    "    return loader_yatc\n",
    "\n",
    "def get_model():\n",
    "    checkpoint_model = torch.load(\"../models/YaTC/YaTC_pretrained_model.pth\")['model']\n",
    "    yatc_frozen_model = models_YaTC.__dict__['TraFormer_YaTC'](\n",
    "            num_classes=1,\n",
    "            drop_path_rate=0.1,\n",
    "        )\n",
    "    interpolate_pos_embed(yatc_frozen_model, checkpoint_model)\n",
    "\n",
    "    #rename norm to fc_norm and delete extra keys\n",
    "    checkpoint_model['fc_norm.bias'] = checkpoint_model['norm.bias']\n",
    "    checkpoint_model['fc_norm.weight'] = checkpoint_model['norm.weight']\n",
    "\n",
    "    keys_to_del = ['mask_token', 'norm.weight', 'norm.bias']\n",
    "    for key in checkpoint_model.keys():\n",
    "        if key.startswith('decoder'):\n",
    "            keys_to_del.append(key)\n",
    "\n",
    "    for key in keys_to_del:\n",
    "        del checkpoint_model[key]\n",
    "\n",
    "    yatc_frozen_model.load_state_dict(checkpoint_model, strict=False)\n",
    "    yatc_frozen_model = yatc_frozen_model.to(\"cuda\")\n",
    "    return yatc_frozen_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859cbf81-5a56-4b15-b9b4-6cf9e0549d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset ImageFolder\n",
      "    Number of datapoints: 1352738\n",
      "    Root location: /dev/shm/data/yatc/train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               Grayscale(num_output_channels=1)\n",
      "               ToTensor()\n",
      "               Normalize(mean=[0.5], std=[0.5])\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "dataloader = get_loader(\"../data/yatc/tmp\", batch_size=BATCH_SIZE)\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "929837b7-44fa-456a-be87-14b92501e68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nheader in YaTC is 80 floats:\\n48:56 are seq\\n56:64 are ack\\n6:8 is total length\\n16:18 is TTL\\nflags are 12, 20, 23\\n68:76 is WSize\\n\\nthen we have 240 floats of payload\\ntotal 320 floats for a single packet\\ntotal 5 packet = 1600 floats\\nreshaped to 40, 40 getting a single image\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "header in YaTC is 80 floats:\n",
    "48:56 are seq\n",
    "56:64 are ack\n",
    "6:8 is total length\n",
    "16:18 is TTL\n",
    "flags are 12, 20, 23\n",
    "68:76 is WSize\n",
    "\n",
    "then we have 240 floats of payload\n",
    "total 320 floats for a single packet\n",
    "total 5 packet = 1600 floats\n",
    "reshaped to 40, 40 getting a single image\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17b57464-410a-4790-bf87-c9eb175d142a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_mask(packet_mask: np.ndarray):\n",
    "    \"320 floats to 1600 and reshape\"\n",
    "    assert packet_mask.shape == (320,)\n",
    "    return np.reshape(np.tile(packet_mask, 5), (40, 40))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2976cff5-c5eb-4daf-add1-c5c90fc85fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(batch, model):\n",
    "    batch = batch.to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        return model.forward_features(batch).mean(dim=1).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f50b05e6-68eb-4e18-9925-9f53e8eb25a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(dataset, model, packet_mask, random=False):\n",
    "    \"takes dataset, model, and binary mask for indices allowed for perturbation, generates random noise tensor with respect to the mask, applies it to the dataset, gets embeddings, returns resulting noise tensor and embeddings\"\n",
    "    embeddings = []\n",
    "    noises = []\n",
    "    total = 0\n",
    "    sims = []\n",
    "    mask_np = repeat_mask(packet_mask)\n",
    "    for batch, _ in tqdm(dataset):\n",
    "        orig_input = batch.clone()\n",
    "        B, C, H, W = batch.size()\n",
    "        noise = torch.empty_like(batch)\n",
    "\n",
    "        if not random:\n",
    "            for h in range(H):\n",
    "                for w in range(W):\n",
    "                    perm = torch.randperm(B)\n",
    "                    noise[:, :, h, w] = batch[perm, :, h, w]\n",
    "        else:\n",
    "            noise = torch.rand((batch.size(0), 1, 40, 40)) * 2 - 1  # from -1 to +1\n",
    "\n",
    "        mask_torch = torch.from_numpy(mask_np).bool().unsqueeze(0).unsqueeze(0).cpu()  # (1, 1, 40, 40)\n",
    "        mask_torch = mask_torch.repeat(batch.size(0), 1, 1, 1)\n",
    "        batch[mask_torch] = noise[mask_torch]\n",
    "        perturbed_features = encode(batch, model)\n",
    "        embeddings.append(perturbed_features)\n",
    "        sims.append((batch == orig_input).float().mean().item())\n",
    "        total += batch.size(0)\n",
    "        if total > LIMIT:\n",
    "            break\n",
    "        \n",
    "    print(f\"Similarity: {np.mean(sims)}\")\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    return embeddings    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "241a62cf-0cfb-418c-89a6-54810c168adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embedding(dataset, model, packet_mask):\n",
    "#     \"takes dataset, model, and binary mask for indices allowed for perturbation, generates random noise tensor with respect to the mask, applies it to the dataset, gets embeddings, returns resulting noise tensor and embeddings\"\n",
    "#     embeddings = []\n",
    "#     noises = []\n",
    "#     mask_np = repeat_mask(packet_mask)\n",
    "#     for batch, _ in tqdm(dataset):\n",
    "#         batch = batch.to(\"cuda\")\n",
    "#         random_tensor = torch.rand((1, 1, 40, 40), device='cuda') * 2 - 1  # from -1 to +1\n",
    "#         mask_torch = torch.from_numpy(mask_np).bool().unsqueeze(0).unsqueeze(0).to(\"cuda\")  # (1, 1, 40, 40)\n",
    "#         random_tensor *= mask_torch\n",
    "#         noises.append(random_tensor)\n",
    "#         batch[mask_torch] = random_tensor[mask_torch]\n",
    "#         perturbed_features = encode(batch, model)\n",
    "#         embeddings.append(perturbed_features)\n",
    "\n",
    "#     embeddings = torch.cat(embeddings)\n",
    "#     noises = torch.cat(noises)\n",
    "#     return embeddings, noises    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf0b164b-be25-4fc6-81d4-7a54fd7fec59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _perturb(packet_mask, random=False):\n",
    "    return get_embedding(dataloader, model, packet_mask, random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc079a9e-1a31-4152-997a-f7f51c152145",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import torch\n",
    "\n",
    "# define correlation calculation function\n",
    "\n",
    "def calculate_correlation(emb_original: torch.Tensor, noise: torch.Tensor, emb_perturbed: torch.Tensor) -> np.ndarray:\n",
    "    '''accepts original embedding, noise, and embedding after perturbation, and calculates similarity correlation between noise and each dimension of the perturbation result'''\n",
    "    emb_diff = emb_perturbed - emb_original\n",
    "\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(emb_perturbed, emb_original).mean()\n",
    "    l2_dist = torch.cdist(emb_perturbed, emb_original, p=2).mean()\n",
    "\n",
    "    emb_diff_np = emb_diff.detach().cpu().numpy()\n",
    "    noise_np = noise.detach().cpu().numpy()\n",
    "    noise_np = np.reshape(noise_np, (noise_np.shape[0], -1))\n",
    "    noise_np = noise_np[:, noise_np.any(axis=0)]  # keep only non zero noise columns effectively removing masked out columns \n",
    "\n",
    "    n_dims = emb_diff_np.shape[1]\n",
    "    cka_scores = np.zeros(n_dims)\n",
    "    \n",
    "    for d in tqdm(range(n_dims)):\n",
    "        # extract the nth column as a 2D array\n",
    "        feature_column = emb_diff_np[:, d].reshape(-1, 1)\n",
    "        cka_scores[d] = feature_space_linear_cka(noise_np, feature_column)\n",
    "\n",
    "    return cos_sim, l2_dist, cka_scores\n",
    "\n",
    "def calculate_correlation(emb_original: torch.Tensor, emb_perturbed: torch.Tensor) -> float:\n",
    "    # simplified\n",
    "    return torch.nn.functional.cosine_similarity(emb_perturbed, emb_original).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "76517369-4e5a-4f1c-8232-dcef3e71ea20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:29<05:24,  3.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# original embeddings\n",
    "mask = np.array([0] * 320)  # zero mask\n",
    "original_embeddings = _perturb(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8d4a00a7-ea2d-4405-a476-5c9123a1fb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(mask):\n",
    "    new_emb, new_noise = _perturb(mask)\n",
    "    cos_sim, l2_dist, correlation = calculate_correlation(original_embeddings, new_noise, new_emb)\n",
    "    top5_sim = np.argsort(correlation)[-5:]\n",
    "    print(f\"Cos sim: {cos_sim}\")\n",
    "    print(f\"L2 distance: {l2_dist}\")\n",
    "    print(f\"Average similarity: {np.mean(correlation)}\")\n",
    "    print(f\"Top 5 indices: {top5_sim[::-1]}\")\n",
    "    print(f\"Top 5 similarity values: {correlation[top5_sim][::-1]}\")\n",
    "\n",
    "# simplified\n",
    "def calculate_similarity(mask):\n",
    "    new_emb = _perturb(mask, random=True)\n",
    "    cos_sim = calculate_correlation(original_embeddings, new_emb)\n",
    "    print(f\"Cos sim for random source perturbation: {cos_sim}\")\n",
    "\n",
    "    new_emb = _perturb(mask)\n",
    "    cos_sim = calculate_correlation(original_embeddings, new_emb)\n",
    "    print(f\"Cos sim for reordered perturbation: {cos_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab9917e2-7212-4b2d-aeb6-427170a538cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:22<04:58,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.25000003695487977\n",
      "Cos sim for random source perturbation: 0.18019339442253113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:27<05:17,  3.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 1.0\n",
      "Cos sim for reordered perturbation: 0.901327908039093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# payload\n",
    "calculate_similarity(np.array([0] * 80 + [1] * 240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e149d3c9-0dac-4899-b839-094c77c67ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:21<04:55,  3.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.949999988079071\n",
      "Cos sim for random source perturbation: 0.6053717136383057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:26<05:13,  3.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 1.0\n",
      "Cos sim for reordered perturbation: 0.9012899994850159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# SEQ/ACK\n",
    "# 48:56 are seq\n",
    "# 56:64 are ack\n",
    "calculate_similarity(np.array([0] * 48 + [1] * 16 + [0] * 16 + [0] * 240))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "738885f0-c439-47f3-b362-ece1ab7e16d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:21<04:54,  3.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9937499761581421\n",
      "Cos sim for random source perturbation: 0.8799247741699219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:26<05:14,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9990293765068055\n",
      "Cos sim for reordered perturbation: 0.8973586559295654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# IP total length\n",
    "# 6:8 is total length\n",
    "calculate_similarity(np.array([0] * 6 + [1] * 2 + [0] * (72 + 240)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2655ef79-319a-404c-aa74-f65ef6d11eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:21<04:55,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9937499761581421\n",
      "Cos sim for random source perturbation: 0.8818549513816833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:27<05:18,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 1.0\n",
      "Cos sim for reordered perturbation: 0.9012280106544495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# IP TTL\n",
    "# 16:18 is TTL\n",
    "calculate_similarity(np.array([0] * 16 + [1] * 2 + [0] * (62 + 240)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc1b9432-e7c5-415d-b020-d5295c93247f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:22<04:58,  3.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9906250238418579\n",
      "Cos sim for random source perturbation: 0.8624997138977051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:27<05:18,  3.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9999999833106995\n",
      "Cos sim for reordered perturbation: 0.9013085961341858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TCP Flags\n",
    "# flags are 12, 20, 23\n",
    "calculate_similarity(np.array(\n",
    "    [0] * 12 + [1] * 1 + \n",
    "    [0] * 7 + [1] * 1 + \n",
    "    [0] * 2 + [1] * 1 +\n",
    "    [0] * (56 + 240)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8982fd6d-2da7-4354-a0d9-6d59e305010c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:21<04:55,  3.40s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 0.9750000238418579\n",
      "Cos sim for random source perturbation: 0.6664440035820007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 24/111 [01:26<05:15,  3.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 1.0\n",
      "Cos sim for reordered perturbation: 0.9014813899993896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# TCP WSize (68:76)\n",
    "calculate_similarity(np.array([0] * 68 + [1] * 8 + [0] * (4 + 240)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5718a9-2b42-4fce-b2af-68de6430990b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "condaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
