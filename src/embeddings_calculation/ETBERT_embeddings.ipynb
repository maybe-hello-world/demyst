{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f7d967f-0733-4760-9488-9f4469245b12",
   "metadata": {},
   "source": [
    "## ET-BERT embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd8248c-df10-49ad-98fb-e9061e5fbee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/pscratch/sd/k/kell/demystifying/ET-BERT/src\n"
     ]
    }
   ],
   "source": [
    "%cd ../models/ET-BERT/src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b716565d-b628-4d04-92f5-936ef198562d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from finetuning.run_classifier import Classifier, read_dataset, load_or_initialize_parameters, count_labels_num, batch_loader\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "from argparse import Namespace\n",
    "from uer.layers import *\n",
    "from uer.encoders import *\n",
    "from uer.utils.constants import *\n",
    "from uer.utils import *\n",
    "from uer.utils.optimizers import *\n",
    "from uer.opts import finetune_opts\n",
    "from uer.utils.config import load_hyperparam\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import threading\n",
    "import pickle\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd9192e-0460-4d9f-819a-d14ed1fc4db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(datafolder, batch_size=64, pretrained_model=\"/dev/shm/pretrained_model_etbert.bin\", limit = 10**30, gpus=4):\n",
    "    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    finetune_opts(parser)\n",
    "    args = []\n",
    "    args += [\"--train_path\", \"dummy\"]\n",
    "    args += [\"--vocab_path\", \"../models/ET-BERT/src/models/encryptd_vocab.txt\"]\n",
    "    args += [\"--dev_path\", \"../models/ET-BERT/cic/test_dataset.tsv\"]\n",
    "    args += [\"--pretrained_model_path\", \"dummy\"]\n",
    "    args = parser.parse_args(args)\n",
    "    args.tokenizer = \"bert\"\n",
    "    args.pooling = \"first\"\n",
    "    args.soft_targets = False\n",
    "    args.topk = 1\n",
    "    args.frozen = False\n",
    "    args.soft_alpha = 0.5\n",
    "    \n",
    "    args = load_hyperparam(args)\n",
    "    args.tokenizer = str2tokenizer[args.tokenizer](args)\n",
    "    args.batch_size = batch_size\n",
    "    args.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    def get_model(args, model_path, train_path):\n",
    "        args.pretrained_model_path = model_path\n",
    "        args.train_path = train_path\n",
    "        args.labels_num = count_labels_num(args.train_path)\n",
    "        model = Classifier(args)\n",
    "        load_or_initialize_parameters(args, model)\n",
    "        model = model.to(args.device)\n",
    "        summary(model)\n",
    "        return model\n",
    "\n",
    "    etbert_frozen_model = get_model(args,pretrained_model, f\"{datafolder}/train_dataset.tsv\")\n",
    "    data_etbert = read_dataset(args, f\"{datafolder}/train_dataset.tsv\")\n",
    "\n",
    "    def encode(model, src_batch, seg_batch):\n",
    "        emb = model.embedding(src_batch, seg_batch)\n",
    "        emb = model.encoder(emb, seg_batch)\n",
    "        emb = emb[:, 0, :]   # pooling = first\n",
    "        return emb\n",
    "\n",
    "    etbert_emb = defaultdict(list)\n",
    "    print(f\"Total: {int(len(data_etbert) / args.batch_size)}\")\n",
    "    \n",
    "    etbert_models = {}\n",
    "    for i in range(gpus):\n",
    "        etbert_models[i] = copy.deepcopy(etbert_frozen_model)\n",
    "        etbert_models[i].to(f\"cuda:{i}\")\n",
    "    print()\n",
    "    \n",
    "    def encode_and_append(batch, model, result_list, i):\n",
    "        src_batch, seg_batch = batch\n",
    "        src_batch = src_batch.to(f\"cuda:{i}\")\n",
    "        seg_batch = seg_batch.to(f\"cuda:{i}\")\n",
    "        with torch.no_grad():\n",
    "            result_list.append(encode(model, src_batch, seg_batch).cpu())\n",
    "        del src_batch, seg_batch, batch\n",
    "    \n",
    "    batch_size = args.batch_size\n",
    "    \n",
    "    src = torch.LongTensor([example[0] for example in data_etbert])\n",
    "    tgt = torch.LongTensor([example[1] for example in data_etbert])\n",
    "    seg = torch.LongTensor([example[2] for example in data_etbert])\n",
    "    \n",
    "    loader = batch_loader(batch_size, src, tgt, seg, None)\n",
    "    iterator = iter(loader)\n",
    "    embeddings = []\n",
    "    try:\n",
    "        for k in tqdm(range(min(int(len(data_etbert) / args.batch_size / gpus)+1, limit))):\n",
    "            etbert_emb = defaultdict(list)\n",
    "            batches = [next(iterator) for i in range(gpus)]            \n",
    "            threads = []\n",
    "            for i in range(gpus):\n",
    "                t = threading.Thread(target=encode_and_append, args=((batches[i][0], batches[i][2]), etbert_models[i], etbert_emb[i], i))\n",
    "                t.start()\n",
    "                threads.append(t)\n",
    "            for t in threads:\n",
    "                t.join()\n",
    "            del batches\n",
    "            embeddings.append(torch.cat([torch.cat(etbert_emb[i]) for i in etbert_emb]))\n",
    "    except StopIteration:\n",
    "        print(\"finished\")\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise\n",
    "\n",
    "    with open(f\"{datafolder}/picked_file_record\") as f:\n",
    "        filenames = f.readlines()\n",
    "\n",
    "    embeddings = torch.cat(embeddings)\n",
    "    filenames = filenames[:embeddings.shape[0]]  # because of our batch selection algorithm, taken number of files should be integer after / gpus / batch_size, so we took first N files where N is resulting tensor shape\n",
    "    print(embeddings.shape, len(filenames))\n",
    "    return embeddings, filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159c533b-f746-468a-863e-127da76061d2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "├─WordPosSegEmbedding: 1-1                         --\n",
      "|    └─Dropout: 2-1                                --\n",
      "|    └─Embedding: 2-2                              46,083,840\n",
      "|    └─Embedding: 2-3                              393,216\n",
      "|    └─Embedding: 2-4                              2,304\n",
      "|    └─LayerNorm: 2-5                              1,536\n",
      "├─TransformerEncoder: 1-2                          --\n",
      "|    └─ModuleList: 2-6                             --\n",
      "|    |    └─TransformerLayer: 3-1                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-2                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-3                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-4                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-5                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-6                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-7                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-8                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-9                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-10                 7,087,872\n",
      "|    |    └─TransformerLayer: 3-11                 7,087,872\n",
      "|    |    └─TransformerLayer: 3-12                 7,087,872\n",
      "├─Linear: 1-3                                      590,592\n",
      "├─Linear: 1-4                                      6,152\n",
      "===========================================================================\n",
      "Total params: 132,132,104\n",
      "Trainable params: 132,132,104\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/global/homes/k/kell/scratch/demystifying/ET-BERT/cic/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(datafolder, batch_size, pretrained_model, limit, gpus)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m     32\u001b[0m etbert_frozen_model \u001b[38;5;241m=\u001b[39m get_model(args,pretrained_model, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatafolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_dataset.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m data_etbert \u001b[38;5;241m=\u001b[39m \u001b[43mread_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatafolder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train_dataset.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(model, src_batch, seg_batch):\n\u001b[1;32m     36\u001b[0m     emb \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39membedding(src_batch, seg_batch)\n",
      "File \u001b[0;32m/pscratch/sd/k/kell/demystifying/ET-BERT/src/finetuning/run_classifier.py:157\u001b[0m, in \u001b[0;36mread_dataset\u001b[0;34m(args, path)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_b\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m columns:  \u001b[38;5;66;03m# Sentence classification.\u001b[39;00m\n\u001b[1;32m    156\u001b[0m     text_a \u001b[38;5;241m=\u001b[39m line[columns[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_a\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[0;32m--> 157\u001b[0m     src \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids([CLS_TOKEN] \u001b[38;5;241m+\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext_a\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    158\u001b[0m     seg \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(src)\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Sentence-pair classification.\u001b[39;00m\n",
      "File \u001b[0;32m/pscratch/sd/k/kell/demystifying/ET-BERT/src/uer/utils/tokenizers.py:228\u001b[0m, in \u001b[0;36mBertTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    227\u001b[0m     split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 228\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sub_token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwordpiece_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(token):\n\u001b[1;32m    230\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(sub_token)\n",
      "File \u001b[0;32m/pscratch/sd/k/kell/demystifying/ET-BERT/src/uer/utils/tokenizers.py:248\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Tokenizes a piece of text.\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m text \u001b[38;5;241m=\u001b[39m convert_to_unicode(text)\n\u001b[0;32m--> 248\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_clean_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[1;32m    256\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenize_chinese_chars(text)\n",
      "File \u001b[0;32m/pscratch/sd/k/kell/demystifying/ET-BERT/src/uer/utils/tokenizers.py:345\u001b[0m, in \u001b[0;36mBasicTokenizer._clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    343\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    344\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 345\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mappend\u001b[49m(char)\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(output)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_embeddings(\"../models/ET-BERT/cic/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238b5a1-6669-458f-b69d-e11fb76ec267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_emb(label):\n",
    "    emb = get_embeddings(f\"../data/{label}\", batch_size=512, gpus=4)\n",
    "    with open(f\"../data/{label}_emb.pkl\", \"bw\") as f:\n",
    "        pickle.dump(emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b899cf23-6513-4944-9c6b-2e7bdf0a8d19",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/dev/shm/data/cross/train_dataset.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msave_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcross\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m, in \u001b[0;36msave_emb\u001b[0;34m(label)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msave_emb\u001b[39m(label):\n\u001b[0;32m----> 2\u001b[0m     emb \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/dev/shm/data/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlabel\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/dev/shm/data/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_emb.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbw\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m         pickle\u001b[38;5;241m.\u001b[39mdump(emb, f)\n",
      "Cell \u001b[0;32mIn[6], line 32\u001b[0m, in \u001b[0;36mget_embeddings\u001b[0;34m(datafolder, batch_size, pretrained_model, limit, gpus)\u001b[0m\n\u001b[1;32m     29\u001b[0m     summary(model)\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m---> 32\u001b[0m etbert_frozen_model \u001b[38;5;241m=\u001b[39m \u001b[43mget_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpretrained_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdatafolder\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/train_dataset.tsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m data_etbert \u001b[38;5;241m=\u001b[39m read_dataset(args, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdatafolder\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/train_dataset.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mencode\u001b[39m(model, src_batch, seg_batch):\n",
      "Cell \u001b[0;32mIn[6], line 25\u001b[0m, in \u001b[0;36mget_embeddings.<locals>.get_model\u001b[0;34m(args, model_path, train_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m args\u001b[38;5;241m.\u001b[39mpretrained_model_path \u001b[38;5;241m=\u001b[39m model_path\n\u001b[1;32m     24\u001b[0m args\u001b[38;5;241m.\u001b[39mtrain_path \u001b[38;5;241m=\u001b[39m train_path\n\u001b[0;32m---> 25\u001b[0m args\u001b[38;5;241m.\u001b[39mlabels_num \u001b[38;5;241m=\u001b[39m \u001b[43mcount_labels_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m model \u001b[38;5;241m=\u001b[39m Classifier(args)\n\u001b[1;32m     27\u001b[0m load_or_initialize_parameters(args, model)\n",
      "File \u001b[0;32m/pscratch/sd/k/kell/demystifying/ET-BERT/src/finetuning/run_classifier.py:76\u001b[0m, in \u001b[0;36mcount_labels_num\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcount_labels_num\u001b[39m(path):\n\u001b[1;32m     75\u001b[0m     labels_set, columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(), {}\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m line_id, line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(f):\n\u001b[1;32m     78\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m line_id \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/dev/shm/data/cross/train_dataset.tsv'"
     ]
    }
   ],
   "source": [
    "save_emb(\"cross\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d6d3c334-176c-4341-9997-e4ada89f3a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1935\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 484/484 [06:14<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([990722, 768]) 990722\n"
     ]
    }
   ],
   "source": [
    "save_emb(\"caida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61083732-3e7c-47ee-9cb6-1de1b87624f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 2408\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 602/603 [08:32<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "torch.Size([1232896, 768]) 1232896\n"
     ]
    }
   ],
   "source": [
    "save_emb(\"cicapt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb2f9da2-8644-4796-8943-8afda286fc25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 856\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 214/215 [02:59<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "torch.Size([438272, 768]) 438272\n"
     ]
    }
   ],
   "source": [
    "save_emb(\"cicids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f450d3c-cb06-4581-a8fa-618a8420a9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 1370\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 342/343 [04:49<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "torch.Size([700416, 768]) 700416\n"
     ]
    }
   ],
   "source": [
    "save_emb(\"mawi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "490460e0-b08f-407f-ad94-0e05e405d80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total: 10\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 10/11 [00:00<00:00, 88.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "torch.Size([10, 768]) 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_emb(\"etbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118a72cf-0e59-4a66-90de-4fdbf2090b84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "Layer (type:depth-idx)                             Param #\n",
      "===========================================================================\n",
      "├─WordPosSegEmbedding: 1-1                         --\n",
      "|    └─Dropout: 2-1                                --\n",
      "|    └─Embedding: 2-2                              46,083,840\n",
      "|    └─Embedding: 2-3                              393,216\n",
      "|    └─Embedding: 2-4                              2,304\n",
      "|    └─LayerNorm: 2-5                              1,536\n",
      "├─TransformerEncoder: 1-2                          --\n",
      "|    └─ModuleList: 2-6                             --\n",
      "|    |    └─TransformerLayer: 3-1                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-2                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-3                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-4                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-5                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-6                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-7                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-8                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-9                  7,087,872\n",
      "|    |    └─TransformerLayer: 3-10                 7,087,872\n",
      "|    |    └─TransformerLayer: 3-11                 7,087,872\n",
      "|    |    └─TransformerLayer: 3-12                 7,087,872\n",
      "├─Linear: 1-3                                      590,592\n",
      "├─Linear: 1-4                                      3,845\n",
      "===========================================================================\n",
      "Total params: 132,129,797\n",
      "Trainable params: 132,129,797\n",
      "Non-trainable params: 0\n",
      "===========================================================================\n",
      "Total: 472\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 472/473 [00:08<00:00, 53.66it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n",
      "torch.Size([472, 768]) 472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "label = \"synth\"\n",
    "emb = get_embeddings(f\"../data/{label}\", batch_size=1, gpus=1)\n",
    "with open(f\"../data/{label}_emb.pkl\", \"bw\") as f:\n",
    "    pickle.dump(emb, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f74d5f-4ab0-444f-8553-c74925179150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "condaenv",
   "language": "python",
   "name": "condaenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
