{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147f4336-4a6e-44c4-ac6f-d546153188bf",
   "metadata": {},
   "source": [
    "## CKA analysis attempt of the same files between different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deaf8a0b-9744-4b72-98c4-cfa5791bab68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78743796-ca04-4c37-b8d2-980f09758744",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappings = {\n",
    "    \"yatc\": lambda x: x.split(\"/\")[-1].removesuffix(\".png\"),\n",
    "    \"etbert\": lambda x: x.split(\"/\")[-1].removesuffix(\".pcap\\n\"),\n",
    "    \"netmamba\": lambda x: x.split(\"/\")[-1].removesuffix(\".png\"),\n",
    "    \"netfound\": lambda x: x.split(\"/\")[-1].removesuffix(\".pcap.6\").removesuffix(\".pcap.17\").removesuffix(\".pcap.1\"),\n",
    "}\n",
    "\n",
    "def map_filenames(filepath, filename_mapping):\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        embeddings, filenames = pickle.load(f)\n",
    "        if embeddings.shape[0] != len(filenames):\n",
    "            print(filepath)\n",
    "            \n",
    "\n",
    "    return embeddings, [filename_mapping(x) for x in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "276c4a66-c678-44ed-a775-779857e88245",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\"cross\", \"caida\", \"cicids\", \"cicapt\", \"mawi\"}\n",
    "models = {\"yatc\", \"etbert\", \"netfound\", \"netmamba\"}\n",
    "\n",
    "def dataset_name(filepath):\n",
    "    for dataset in datasets:\n",
    "        if dataset in filepath:\n",
    "            return dataset\n",
    "    return None\n",
    "\n",
    "result = {\n",
    "    key: {k: None for k in models}\n",
    "    for key in datasets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9e16adf-29db-4311-9bd5-080a40972e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    directory = f'../data/{model}/'\n",
    "    for filename in os.listdir(directory):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        dataset = dataset_name(filepath)\n",
    "        if dataset is None:\n",
    "            continue\n",
    "        result[dataset][model] = map_filenames(filepath, mappings[model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa5fa4ab-6e11-4a72-a9c5-e27ffe24ea96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caida 1351680 989078\n",
      "cross 44544 27232\n",
      "cicapt 1576960 1232708\n",
      "cicids 557056 435632\n",
      "mawi 997376 698286\n"
     ]
    }
   ],
   "source": [
    "# verification that filenames are preprocessed in the correct way - the intersection of filenames between models should not be much less than a number of files of any of them\n",
    "for dataset in datasets:\n",
    "    filenames = [set(result[dataset][model][1]) for model in models]\n",
    "    print(dataset, len(filenames[0]), len(set.intersection(*filenames)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dafc0c9-28f2-4eb9-b822-6c473631da71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_embeddings(result, models, datasets):\n",
    "    aligned_embeddings = {dataset: {} for dataset in datasets}\n",
    "\n",
    "    for dataset in datasets:\n",
    "        # Compute intersection of filenames across all models\n",
    "        filename_sets = [set(result[dataset][model][1]) for model in models]\n",
    "        common_filenames = set.intersection(*filename_sets)\n",
    "        \n",
    "        # Create a consistent ordering for common filenames\n",
    "        sorted_common_filenames = sorted(common_filenames)\n",
    "\n",
    "        for model in models:\n",
    "            embeddings, filenames = result[dataset][model]\n",
    "\n",
    "            # Map filenames to indices for fast lookup\n",
    "            filename_to_index = {filename: idx for idx, filename in enumerate(filenames)}\n",
    "\n",
    "            # Check if embeddings size matches the filenames length\n",
    "            assert embeddings.shape[0] == len(filenames), \\\n",
    "                f\"Embeddings size ({embeddings.shape[0]}) and filenames length ({len(filenames)}) mismatch for model {model}\"\n",
    "\n",
    "            # Filter out filenames that might have been incorrectly included\n",
    "            aligned_indices = [filename_to_index[fn] for fn in sorted_common_filenames if fn in filename_to_index]\n",
    "\n",
    "            # Convert to tensor for indexing\n",
    "            aligned_indices_tensor = torch.tensor(aligned_indices, dtype=torch.long)\n",
    "\n",
    "            # Select embeddings safely using aligned indices\n",
    "            aligned_tensor = embeddings[aligned_indices_tensor]\n",
    "\n",
    "            # Store aligned tensor\n",
    "            aligned_embeddings[dataset][model] = aligned_tensor\n",
    "\n",
    "    return aligned_embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18fc94a1-05a1-4adb-9266-f663769c0234",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m aligned_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m, in \u001b[0;36mpreprocess_embeddings\u001b[1;34m(result, models, datasets)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(filenames), \\\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and filenames length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filenames)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) mismatch for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Filter out filenames that might have been incorrectly included\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m aligned_indices \u001b[38;5;241m=\u001b[39m [filename_to_index[fn] \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m sorted_common_filenames \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m filename_to_index]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Convert to tensor for indexing\u001b[39;00m\n\u001b[0;32m     26\u001b[0m aligned_indices_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(aligned_indices, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(filenames), \\\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEmbeddings size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00membeddings\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) and filenames length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filenames)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) mismatch for model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Filter out filenames that might have been incorrectly included\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m aligned_indices \u001b[38;5;241m=\u001b[39m [\u001b[43mfilename_to_index\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m sorted_common_filenames \u001b[38;5;28;01mif\u001b[39;00m fn \u001b[38;5;129;01min\u001b[39;00m filename_to_index]\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Convert to tensor for indexing\u001b[39;00m\n\u001b[0;32m     26\u001b[0m aligned_indices_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(aligned_indices, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aligned_embeddings = preprocess_embeddings(result, models, datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b88b9a-e629-4c12-b39f-b84b14f36758",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def gram_linear(x):\n",
    "    \"\"\"Compute Gram (kernel) matrix for a linear kernel.\n",
    "\n",
    "    Args:\n",
    "        x: A num_examples x num_features matrix of features.\n",
    "\n",
    "    Returns:\n",
    "        A num_examples x num_examples Gram matrix of examples.\n",
    "    \"\"\"\n",
    "    return x.dot(x.T)\n",
    "\n",
    "\n",
    "def gram_rbf(x, threshold=1.0):\n",
    "    \"\"\"Compute Gram (kernel) matrix for an RBF kernel.\n",
    "\n",
    "    Args:\n",
    "        x: A num_examples x num_features matrix of features.\n",
    "        threshold: Fraction of median Euclidean distance to use as RBF kernel\n",
    "        bandwidth. (This is the heuristic we use in the paper. There are other\n",
    "        possible ways to set the bandwidth; we didn't try them.)\n",
    "\n",
    "    Returns:\n",
    "        A num_examples x num_examples Gram matrix of examples.\n",
    "    \"\"\"\n",
    "    dot_products = x.dot(x.T)\n",
    "    sq_norms = np.diag(dot_products)\n",
    "    sq_distances = -2 * dot_products + sq_norms[:, None] + sq_norms[None, :]\n",
    "    sq_median_distance = np.median(sq_distances)\n",
    "    return np.exp(-sq_distances / (2 * threshold ** 2 * sq_median_distance))\n",
    "\n",
    "\n",
    "def center_gram(gram, unbiased=False):\n",
    "    \"\"\"Center a symmetric Gram matrix.\n",
    "\n",
    "    This is equvialent to centering the (possibly infinite-dimensional) features\n",
    "    induced by the kernel before computing the Gram matrix.\n",
    "\n",
    "    Args:\n",
    "        gram: A num_examples x num_examples symmetric matrix.\n",
    "        unbiased: Whether to adjust the Gram matrix in order to compute an unbiased\n",
    "        estimate of HSIC. Note that this estimator may be negative.\n",
    "\n",
    "    Returns:\n",
    "        A symmetric matrix with centered columns and rows.\n",
    "    \"\"\"\n",
    "    if not np.allclose(gram, gram.T):\n",
    "        raise ValueError('Input must be a symmetric matrix.')\n",
    "    gram = gram.copy()\n",
    "\n",
    "    if unbiased:\n",
    "        # This formulation of the U-statistic, from Szekely, G. J., & Rizzo, M.\n",
    "        # L. (2014). Partial distance correlation with methods for dissimilarities.\n",
    "        # The Annals of Statistics, 42(6), 2382-2412, seems to be more numerically\n",
    "        # stable than the alternative from Song et al. (2007).\n",
    "        n = gram.shape[0]\n",
    "        np.fill_diagonal(gram, 0)\n",
    "        means = np.sum(gram, 0, dtype=np.float64) / (n - 2)\n",
    "        means -= np.sum(means) / (2 * (n - 1))\n",
    "        gram -= means[:, None]\n",
    "        gram -= means[None, :]\n",
    "        np.fill_diagonal(gram, 0)\n",
    "    else:\n",
    "        means = np.mean(gram, 0, dtype=np.float64)\n",
    "        means -= np.mean(means) / 2\n",
    "        gram -= means[:, None]\n",
    "        gram -= means[None, :]\n",
    "\n",
    "    return gram\n",
    "\n",
    "\n",
    "def cka(gram_x, gram_y, debiased=False):\n",
    "    \"\"\"Compute CKA.\n",
    "\n",
    "    Args:\n",
    "        gram_x: A num_examples x num_examples Gram matrix.\n",
    "        gram_y: A num_examples x num_examples Gram matrix.\n",
    "        debiased: Use unbiased estimator of HSIC. CKA may still be biased.\n",
    "\n",
    "    Returns:\n",
    "        The value of CKA between X and Y.\n",
    "    \"\"\"\n",
    "    gram_x = center_gram(gram_x, unbiased=debiased)\n",
    "    gram_y = center_gram(gram_y, unbiased=debiased)\n",
    "\n",
    "    # Note: To obtain HSIC, this should be divided by (n-1)**2 (biased variant) or\n",
    "    # n*(n-3) (unbiased variant), but this cancels for CKA.\n",
    "    scaled_hsic = gram_x.ravel().dot(gram_y.ravel())\n",
    "\n",
    "    normalization_x = np.linalg.norm(gram_x)\n",
    "    normalization_y = np.linalg.norm(gram_y)\n",
    "    return scaled_hsic / (normalization_x * normalization_y)\n",
    "\n",
    "\n",
    "def _debiased_dot_product_similarity_helper(\n",
    "    xty, sum_squared_rows_x, sum_squared_rows_y, squared_norm_x, squared_norm_y,\n",
    "    n):\n",
    "  \"\"\"Helper for computing debiased dot product similarity (i.e. linear HSIC).\"\"\"\n",
    "  # This formula can be derived by manipulating the unbiased estimator from\n",
    "  # Song et al. (2007).\n",
    "  return (\n",
    "      xty - n / (n - 2.) * sum_squared_rows_x.dot(sum_squared_rows_y)\n",
    "      + squared_norm_x * squared_norm_y / ((n - 1) * (n - 2)))\n",
    "\n",
    "\n",
    "def feature_space_linear_cka(features_x, features_y, debiased=False):\n",
    "    \"\"\"Compute CKA with a linear kernel, in feature space.\n",
    "\n",
    "    This is typically faster than computing the Gram matrix when there are fewer\n",
    "    features than examples.\n",
    "\n",
    "    Args:\n",
    "        features_x: A num_examples x num_features matrix of features.\n",
    "        features_y: A num_examples x num_features matrix of features.\n",
    "        debiased: Use unbiased estimator of dot product similarity. CKA may still be\n",
    "        biased. Note that this estimator may be negative.\n",
    "\n",
    "    Returns:\n",
    "        The value of CKA between X and Y.\n",
    "    \"\"\"\n",
    "    features_x = features_x - np.mean(features_x, 0, keepdims=True)\n",
    "    features_y = features_y - np.mean(features_y, 0, keepdims=True)\n",
    "\n",
    "    dot_product_similarity = np.linalg.norm(features_x.T.dot(features_y)) ** 2\n",
    "    normalization_x = np.linalg.norm(features_x.T.dot(features_x))\n",
    "    normalization_y = np.linalg.norm(features_y.T.dot(features_y))\n",
    "\n",
    "    if debiased:\n",
    "        n = features_x.shape[0]\n",
    "        # Equivalent to np.sum(features_x ** 2, 1) but avoids an intermediate array.\n",
    "        sum_squared_rows_x = np.einsum('ij,ij->i', features_x, features_x)\n",
    "        sum_squared_rows_y = np.einsum('ij,ij->i', features_y, features_y)\n",
    "        squared_norm_x = np.sum(sum_squared_rows_x)\n",
    "        squared_norm_y = np.sum(sum_squared_rows_y)\n",
    "\n",
    "        dot_product_similarity = _debiased_dot_product_similarity_helper(\n",
    "            dot_product_similarity, sum_squared_rows_x, sum_squared_rows_y,\n",
    "            squared_norm_x, squared_norm_y, n)\n",
    "        normalization_x = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "            normalization_x ** 2, sum_squared_rows_x, sum_squared_rows_x,\n",
    "            squared_norm_x, squared_norm_x, n))\n",
    "        normalization_y = np.sqrt(_debiased_dot_product_similarity_helper(\n",
    "            normalization_y ** 2, sum_squared_rows_y, sum_squared_rows_y,\n",
    "            squared_norm_y, squared_norm_y, n))\n",
    "\n",
    "    return dot_product_similarity / (normalization_x * normalization_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4df6d3-866a-44bc-9211-7accd081c1ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "caida\n",
      "Linear CKA between models yatc and netmamba: 0.3365003673400656\n",
      "Linear CKA between models yatc and etbert: 0.365893825228769\n",
      "Linear CKA between models yatc and netfound: 0.2653822777484194\n",
      "Linear CKA between models netmamba and etbert: 0.43889288062224685\n",
      "Linear CKA between models netmamba and netfound: 0.6403622709121128\n",
      "Linear CKA between models etbert and netfound: 0.3841278084302739\n",
      "----------------------------------------------------------------------------------------------------\n",
      "cicapt\n",
      "Linear CKA between models yatc and netmamba: 0.0008857271169928451\n",
      "Linear CKA between models yatc and etbert: 0.0014339914055425346\n",
      "Linear CKA between models yatc and netfound: 0.0036548199156202785\n",
      "Linear CKA between models netmamba and etbert: 0.002344266534687644\n",
      "Linear CKA between models netmamba and netfound: 0.05766893786832997\n",
      "Linear CKA between models etbert and netfound: 0.015383305018852851\n",
      "----------------------------------------------------------------------------------------------------\n",
      "cross\n",
      "Linear CKA between models yatc and netmamba: 0.00926593483007248\n",
      "Linear CKA between models yatc and etbert: 0.015587023795198338\n",
      "Linear CKA between models yatc and netfound: 0.05989552615135249\n",
      "Linear CKA between models netmamba and etbert: 0.00856379139178801\n",
      "Linear CKA between models netmamba and netfound: 0.044856718757947764\n",
      "Linear CKA between models etbert and netfound: 0.052236709272405965\n",
      "----------------------------------------------------------------------------------------------------\n",
      "cicids\n",
      "Linear CKA between models yatc and netmamba: 0.1395957810328577\n",
      "Linear CKA between models yatc and etbert: 0.4753557426094979\n",
      "Linear CKA between models yatc and netfound: 0.35596875050639676\n",
      "Linear CKA between models netmamba and etbert: 0.14566179480480665\n",
      "Linear CKA between models netmamba and netfound: 0.17330034786175677\n",
      "Linear CKA between models etbert and netfound: 0.3214793444329415\n",
      "----------------------------------------------------------------------------------------------------\n",
      "mawi\n",
      "Linear CKA between models yatc and netmamba: 0.1758976750715692\n",
      "Linear CKA between models yatc and etbert: 0.1627724603000925\n",
      "Linear CKA between models yatc and netfound: 0.16323572365943964\n",
      "Linear CKA between models netmamba and etbert: 0.4805907553326318\n",
      "Linear CKA between models netmamba and netfound: 0.7743462234870186\n",
      "Linear CKA between models etbert and netfound: 0.47501187485392377\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(dataset)\n",
    "    embeddings = aligned_embeddings[dataset]\n",
    "    \n",
    "    unique_pairs = list(itertools.combinations(models, 2))\n",
    "    \n",
    "    for model1, model2 in unique_pairs:\n",
    "        cka_from_features = feature_space_linear_cka(embeddings[model1].numpy(), embeddings[model2].numpy(), debiased=False)\n",
    "        print(f'Linear CKA between models {model1} and {model2}: {cka_from_features}')\n",
    "    print('-' * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6862ad-e99b-49bb-a487-0b4f9763dae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
